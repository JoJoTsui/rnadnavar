{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VCF Benchmarking Statistics Analysis\n",
    "\n",
    "This notebook performs comprehensive VCF statistics on CASTLE benchmarking truth sets for DeepSomatic model training and evaluation.\n",
    "\n",
    "## Analysis Overview\n",
    "- **Whole-Landscape Analysis**: Cross-sample comparisons and aggregate statistics\n",
    "- **Per-Sample Analysis**: Individual sample characterization\n",
    "- **Statistics Included**:\n",
    "  - Variant type distribution (SNP/INDEL counts and percentages)\n",
    "  - Chromosome distribution (natural sort order, counts and percentages)\n",
    "  - FILTER label statistics\n",
    "  - Ti/Tv ratio (transition/transversion)\n",
    "  - Base change spectrum (6-type mutation categories)\n",
    "  - Indel size distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Import libraries and configure paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from cyvcf2 import VCF\n",
    "\n",
    "# Paths\n",
    "BENCHMARKING_BASE = Path('/t9k/mnt/WorkSpace/data/ngs/xuzhenyu/CASTLE/benchmarking')\n",
    "OUTPUT_DIR = Path('/t9k/mnt/hdd/work/Vax/pipeline/rnadnavar/notebook/castle_statistics_output')\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print('âœ“ Setup complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Define benchmarking categories, samples, and VCF file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking categories\n",
    "BENCHMARK_CATEGORIES = [\n",
    "    'DeepSomatic_multicancer-model_benchmark',\n",
    "    'DeepSomatic_HCC1395-model_benchmark'\n",
    "]\n",
    "\n",
    "# Expected samples\n",
    "SAMPLES = ['H1437', 'H2009', 'HCC1395', 'HCC1937', 'HCC1954', 'HG008', 'Hs578']\n",
    "\n",
    "# VCF file paths\n",
    "VCF_PATHS = [\n",
    "    # HCC1395-model benchmark\n",
    "    BENCHMARKING_BASE / 'DeepSomatic_HCC1395-model_benchmark/vcfs/H1437_DeepSomatic_HCC1395-model.vcf.gz',\n",
    "    BENCHMARKING_BASE / 'DeepSomatic_HCC1395-model_benchmark/vcfs/H2009_DeepSomatic_HCC1395-model.vcf.gz',\n",
    "    BENCHMARKING_BASE / 'DeepSomatic_HCC1395-model_benchmark/vcfs/HCC1395_DeepSomatic_HCC1395-model.vcf.gz',\n",
    "    BENCHMARKING_BASE / 'DeepSomatic_HCC1395-model_benchmark/vcfs/HCC1937_DeepSomatic_HCC1395-model.vcf.gz',\n",
    "    BENCHMARKING_BASE / 'DeepSomatic_HCC1395-model_benchmark/vcfs/HCC1954_DeepSomatic_HCC1395-model.vcf.gz',\n",
    "    # multicancer-model benchmark\n",
    "    BENCHMARKING_BASE / 'DeepSomatic_multicancer-model_benchmark/vcfs/H1437_DeepSomatic_multicancer-model.vcf.gz',\n",
    "    BENCHMARKING_BASE / 'DeepSomatic_multicancer-model_benchmark/vcfs/H2009_DeepSomatic_multicancer-model.vcf.gz',\n",
    "    BENCHMARKING_BASE / 'DeepSomatic_multicancer-model_benchmark/vcfs/HCC1395_DeepSomatic_multicancer-model.vcf.gz',\n",
    "    BENCHMARKING_BASE / 'DeepSomatic_multicancer-model_benchmark/vcfs/HCC1937_DeepSomatic_multicancer-model.vcf.gz',\n",
    "    BENCHMARKING_BASE / 'DeepSomatic_multicancer-model_benchmark/vcfs/HCC1954_DeepSomatic_multicancer-model.vcf.gz',\n",
    "]\n",
    "\n",
    "# Validate paths\n",
    "valid_vcfs = [p for p in VCF_PATHS if p.exists()]\n",
    "missing_vcfs = [p for p in VCF_PATHS if not p.exists()]\n",
    "\n",
    "print(f'âœ“ Found {len(valid_vcfs)} VCF files')\n",
    "if missing_vcfs:\n",
    "    print(f'âš ï¸  Missing {len(missing_vcfs)} files:')\n",
    "    for p in missing_vcfs[:5]:\n",
    "        print(f'   - {p.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions\n",
    "\n",
    "### 3.1 Chromosome Natural Sort Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def natural_sort_chromosomes(chromosomes: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Sort chromosomes in natural order: 1, 2, ..., 22, X, Y, M/MT.\n",
    "    Handles both 'chr' prefixed and non-prefixed formats.\n",
    "    \"\"\"\n",
    "    def chrom_sort_key(chrom: str) -> Tuple:\n",
    "        # Remove 'chr' prefix if present\n",
    "        c = chrom.lower().replace('chr', '')\n",
    "        \n",
    "        # Define order for special chromosomes\n",
    "        special_order = {'x': 23, 'y': 24, 'm': 25, 'mt': 25}\n",
    "        \n",
    "        if c in special_order:\n",
    "            return (special_order[c], 0, chrom)\n",
    "        \n",
    "        # Try to parse as integer\n",
    "        try:\n",
    "            num = int(c)\n",
    "            return (num, 0, chrom)\n",
    "        except ValueError:\n",
    "            # Unknown chromosome, sort after known ones\n",
    "            return (100, 0, chrom)\n",
    "    \n",
    "    return sorted(chromosomes, key=chrom_sort_key)\n",
    "\n",
    "\n",
    "def get_chromosome_order() -> List[str]:\n",
    "    \"\"\"Return standard chromosome order for consistent plotting.\"\"\"\n",
    "    return [f'chr{i}' for i in range(1, 23)] + ['chrX', 'chrY', 'chrM']\n",
    "\n",
    "\n",
    "# Test natural sort\n",
    "test_chroms = ['chr2', 'chr10', 'chrX', 'chr1', 'chrY', 'chr22', 'chrM']\n",
    "print(f'Natural sort test: {natural_sort_chromosomes(test_chroms)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_vcf_header(vcf_path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Inspect VCF header to discover available fields.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with 'info_fields', 'format_fields', 'filter_fields', 'samples'\n",
    "    \"\"\"\n",
    "    vcf = VCF(str(vcf_path))\n",
    "    samples = list(vcf.samples)  # Get samples before iterating\n",
    "    \n",
    "    info_fields = {}\n",
    "    format_fields = {}\n",
    "    filter_fields = {}\n",
    "    \n",
    "    for field in vcf.header_iter():\n",
    "        header_type = field['HeaderType']\n",
    "        \n",
    "        if header_type == 'INFO':\n",
    "            try:\n",
    "                info_fields[field['ID']] = {\n",
    "                    'type': field['Type'],\n",
    "                    'description': field['Description']\n",
    "                }\n",
    "            except (KeyError, TypeError):\n",
    "                info_fields[field['ID']] = {'type': 'Unknown', 'description': ''}\n",
    "                \n",
    "        elif header_type == 'FORMAT':\n",
    "            try:\n",
    "                format_fields[field['ID']] = {\n",
    "                    'type': field['Type'],\n",
    "                    'description': field['Description']\n",
    "                }\n",
    "            except (KeyError, TypeError):\n",
    "                format_fields[field['ID']] = {'type': 'Unknown', 'description': ''}\n",
    "                \n",
    "        elif header_type == 'FILTER':\n",
    "            try:\n",
    "                filter_fields[field['ID']] = field['Description']\n",
    "            except (KeyError, TypeError):\n",
    "                filter_fields[field['ID']] = ''\n",
    "    \n",
    "    vcf.close()\n",
    "    \n",
    "    return {\n",
    "        'info_fields': info_fields,\n",
    "        'format_fields': format_fields,\n",
    "        'filter_fields': filter_fields,\n",
    "        'samples': samples,\n",
    "        'path': vcf_path\n",
    "    }\n",
    "\n",
    "\n",
    "def summarize_vcf_fields(vcf_paths: List[Path]) -> pd.DataFrame:\n",
    "    \"\"\"Summarize available fields across all VCF files.\"\"\"\n",
    "    summaries = []\n",
    "    \n",
    "    for vcf_path in vcf_paths:\n",
    "        if not vcf_path.exists():\n",
    "            continue\n",
    "        \n",
    "        header = inspect_vcf_header(vcf_path)\n",
    "        summaries.append({\n",
    "            'File': vcf_path.name,\n",
    "            'INFO_Fields': len(header['info_fields']),\n",
    "            'FORMAT_Fields': len(header['format_fields']),\n",
    "            'FILTER_Fields': len(header['filter_fields']),\n",
    "            'Samples': len(header['samples']),\n",
    "            'Has_GT': 'GT' in header['format_fields'],\n",
    "            'Has_DP': 'DP' in header['format_fields'],\n",
    "            'Has_AF': 'AF' in header['format_fields'],\n",
    "            'Has_AD': 'AD' in header['format_fields'],\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(summaries)\n",
    "\n",
    "\n",
    "# Inspect first VCF to understand structure\n",
    "if valid_vcfs:\n",
    "    sample_header = inspect_vcf_header(valid_vcfs[0])\n",
    "    print(f\"Sample VCF: {valid_vcfs[0].name}\")\n",
    "    print(f\"  INFO fields: {list(sample_header['info_fields'].keys())[:10]}...\")\n",
    "    print(f\"  FORMAT fields: {list(sample_header['format_fields'].keys())}\")\n",
    "    print(f\"  FILTER fields: {list(sample_header['filter_fields'].keys())}\")\n",
    "    print(f\"  Samples: {sample_header['samples']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Core Statistics Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VCFBenchmarkStats:\n",
    "    \"\"\"\n",
    "    Extract comprehensive statistics from a VCF file.\n",
    "    Designed to work with any VCF format (universal, no pipeline-specific dependencies).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Base change categories (complement-collapsed for standard mutation spectrum)\n",
    "    BASE_CHANGE_CATEGORIES = {\n",
    "        ('C', 'A'): 'C>A', ('G', 'T'): 'C>A',\n",
    "        ('C', 'G'): 'C>G', ('G', 'C'): 'C>G',\n",
    "        ('C', 'T'): 'C>T', ('G', 'A'): 'C>T',\n",
    "        ('T', 'A'): 'T>A', ('A', 'T'): 'T>A',\n",
    "        ('T', 'C'): 'T>C', ('A', 'G'): 'T>C',\n",
    "        ('T', 'G'): 'T>G', ('A', 'C'): 'T>G',\n",
    "    }\n",
    "    \n",
    "    TRANSITIONS = {('A', 'G'), ('G', 'A'), ('C', 'T'), ('T', 'C')}\n",
    "    TRANSVERSIONS = {('A', 'C'), ('C', 'A'), ('A', 'T'), ('T', 'A'),\n",
    "                     ('G', 'C'), ('C', 'G'), ('G', 'T'), ('T', 'G')}\n",
    "    \n",
    "    # Indel size bins\n",
    "    INDEL_SIZE_BINS = ['1', '2', '3', '4-14', '15-29', '30-49', '>50']\n",
    "    \n",
    "    def __init__(self, vcf_path: Path):\n",
    "        self.vcf_path = vcf_path\n",
    "        self.stats = None\n",
    "    \n",
    "    @staticmethod\n",
    "    def categorize_indel_size(size: int) -> str:\n",
    "        \"\"\"\n",
    "        Categorize indel size into bins.\n",
    "        \n",
    "        Args:\n",
    "            size: Indel size (positive for insertion, negative for deletion)\n",
    "        \n",
    "        Returns:\n",
    "            Bin label: '1', '2', '3', '4-14', '15-29', '30-49', '>50'\n",
    "        \"\"\"\n",
    "        abs_size = abs(size)\n",
    "        \n",
    "        if abs_size == 1:\n",
    "            return '1'\n",
    "        elif abs_size == 2:\n",
    "            return '2'\n",
    "        elif abs_size == 3:\n",
    "            return '3'\n",
    "        elif 4 <= abs_size <= 14:\n",
    "            return '4-14'\n",
    "        elif 15 <= abs_size <= 29:\n",
    "            return '15-29'\n",
    "        elif 30 <= abs_size <= 49:\n",
    "            return '30-49'\n",
    "        else:  # >= 50\n",
    "            return '>50'\n",
    "        \n",
    "    def extract_all_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Extract all statistics from the VCF file.\"\"\"\n",
    "        vcf = VCF(str(self.vcf_path))\n",
    "        \n",
    "        stats = {\n",
    "            'total': 0,\n",
    "            'snps': 0,\n",
    "            'indels': 0,\n",
    "            'other': 0,\n",
    "            'chromosomes': defaultdict(int),\n",
    "            'filters': defaultdict(int),\n",
    "            'transitions': 0,\n",
    "            'transversions': 0,\n",
    "            'base_changes': defaultdict(int),\n",
    "            'indel_sizes': [],\n",
    "            'qual_scores': [],\n",
    "        }\n",
    "        \n",
    "        for variant in vcf:\n",
    "            stats['total'] += 1\n",
    "            \n",
    "            # Chromosome\n",
    "            chrom = variant.CHROM\n",
    "            stats['chromosomes'][chrom] += 1\n",
    "            \n",
    "            # FILTER\n",
    "            filt = variant.FILTER if variant.FILTER else 'PASS'\n",
    "            stats['filters'][filt] += 1\n",
    "            \n",
    "            # QUAL score\n",
    "            if variant.QUAL is not None and variant.QUAL != -1:\n",
    "                stats['qual_scores'].append(variant.QUAL)\n",
    "            \n",
    "            # Variant type\n",
    "            ref = variant.REF.upper()\n",
    "            alt = variant.ALT[0].upper() if variant.ALT else ''\n",
    "            \n",
    "            if variant.is_snp:\n",
    "                stats['snps'] += 1\n",
    "                \n",
    "                # Ti/Tv calculation\n",
    "                if (ref, alt) in self.TRANSITIONS:\n",
    "                    stats['transitions'] += 1\n",
    "                elif (ref, alt) in self.TRANSVERSIONS:\n",
    "                    stats['transversions'] += 1\n",
    "                \n",
    "                # Base change spectrum\n",
    "                if (ref, alt) in self.BASE_CHANGE_CATEGORIES:\n",
    "                    category = self.BASE_CHANGE_CATEGORIES[(ref, alt)]\n",
    "                    stats['base_changes'][category] += 1\n",
    "                    \n",
    "            elif variant.is_indel:\n",
    "                stats['indels'] += 1\n",
    "                # Indel size: positive = insertion, negative = deletion\n",
    "                indel_size = len(alt) - len(ref)\n",
    "                stats['indel_sizes'].append(indel_size)\n",
    "            else:\n",
    "                stats['other'] += 1\n",
    "        \n",
    "        vcf.close()\n",
    "        \n",
    "        # Convert defaultdicts to regular dicts\n",
    "        stats['chromosomes'] = dict(stats['chromosomes'])\n",
    "        stats['filters'] = dict(stats['filters'])\n",
    "        stats['base_changes'] = dict(stats['base_changes'])\n",
    "        \n",
    "        # Calculate Ti/Tv ratio\n",
    "        if stats['transversions'] > 0:\n",
    "            stats['titv_ratio'] = stats['transitions'] / stats['transversions']\n",
    "        else:\n",
    "            stats['titv_ratio'] = None\n",
    "        \n",
    "        self.stats = stats\n",
    "        return stats\n",
    "    \n",
    "    def get_variant_type_summary(self) -> pd.DataFrame:\n",
    "        \"\"\"Get variant type counts and percentages.\"\"\"\n",
    "        if not self.stats:\n",
    "            self.extract_all_stats()\n",
    "        \n",
    "        total = self.stats['total']\n",
    "        data = {\n",
    "            'Type': ['SNP', 'INDEL', 'Other', 'Total'],\n",
    "            'Count': [\n",
    "                self.stats['snps'],\n",
    "                self.stats['indels'],\n",
    "                self.stats['other'],\n",
    "                total\n",
    "            ],\n",
    "            'Percentage': [\n",
    "                self.stats['snps'] / total * 100 if total > 0 else 0,\n",
    "                self.stats['indels'] / total * 100 if total > 0 else 0,\n",
    "                self.stats['other'] / total * 100 if total > 0 else 0,\n",
    "                100.0\n",
    "            ]\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        df['Percentage'] = df['Percentage'].round(2)\n",
    "        return df\n",
    "    \n",
    "    def get_chromosome_summary(self) -> pd.DataFrame:\n",
    "        \"\"\"Get chromosome distribution with counts and percentages (natural sort).\"\"\"\n",
    "        if not self.stats:\n",
    "            self.extract_all_stats()\n",
    "        \n",
    "        total = self.stats['total']\n",
    "        chroms = natural_sort_chromosomes(list(self.stats['chromosomes'].keys()))\n",
    "        \n",
    "        data = {\n",
    "            'Chromosome': chroms,\n",
    "            'Count': [self.stats['chromosomes'][c] for c in chroms],\n",
    "            'Percentage': [self.stats['chromosomes'][c] / total * 100 if total > 0 else 0 for c in chroms]\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        df['Percentage'] = df['Percentage'].round(2)\n",
    "        return df\n",
    "    \n",
    "    def get_filter_summary(self) -> pd.DataFrame:\n",
    "        \"\"\"Get FILTER label distribution with counts and percentages.\"\"\"\n",
    "        if not self.stats:\n",
    "            self.extract_all_stats()\n",
    "        \n",
    "        total = self.stats['total']\n",
    "        filters = sorted(self.stats['filters'].keys())\n",
    "        \n",
    "        data = {\n",
    "            'Filter': filters,\n",
    "            'Count': [self.stats['filters'][f] for f in filters],\n",
    "            'Percentage': [self.stats['filters'][f] / total * 100 if total > 0 else 0 for f in filters]\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        df['Percentage'] = df['Percentage'].round(2)\n",
    "        return df.sort_values('Count', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    def get_base_change_summary(self) -> pd.DataFrame:\n",
    "        \"\"\"Get 6-type base change spectrum with counts and percentages.\"\"\"\n",
    "        if not self.stats:\n",
    "            self.extract_all_stats()\n",
    "        \n",
    "        snp_total = self.stats['snps']\n",
    "        categories = ['C>A', 'C>G', 'C>T', 'T>A', 'T>C', 'T>G']\n",
    "        \n",
    "        data = {\n",
    "            'Base_Change': categories,\n",
    "            'Count': [self.stats['base_changes'].get(c, 0) for c in categories],\n",
    "            'Percentage': [\n",
    "                self.stats['base_changes'].get(c, 0) / snp_total * 100 if snp_total > 0 else 0\n",
    "                for c in categories\n",
    "            ]\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        df['Percentage'] = df['Percentage'].round(2)\n",
    "        return df\n",
    "    \n",
    "    def indel_stats_by_bin(self) -> Dict[str, Dict[str, int]]:\n",
    "        \"\"\"\n",
    "        Get indel statistics grouped by size bins.\n",
    "        \n",
    "        Returns:\n",
    "            Dict with 'insertions' and 'deletions' keys, each containing bin -> count mapping\n",
    "        \"\"\"\n",
    "        if not self.stats:\n",
    "            self.extract_all_stats()\n",
    "        \n",
    "        sizes = self.stats['indel_sizes']\n",
    "        \n",
    "        # Initialize bins\n",
    "        bin_stats = {\n",
    "            'insertions': {bin_label: 0 for bin_label in self.INDEL_SIZE_BINS},\n",
    "            'deletions': {bin_label: 0 for bin_label in self.INDEL_SIZE_BINS}\n",
    "        }\n",
    "        \n",
    "        # Count indels by bin\n",
    "        for size in sizes:\n",
    "            bin_label = self.categorize_indel_size(size)\n",
    "            if size > 0:\n",
    "                bin_stats['insertions'][bin_label] += 1\n",
    "            else:\n",
    "                bin_stats['deletions'][bin_label] += 1\n",
    "        \n",
    "        return bin_stats\n",
    "    \n",
    "    def get_indel_size_summary(self) -> pd.DataFrame:\n",
    "        \"\"\"Get indel size distribution summary.\"\"\"\n",
    "        if not self.stats:\n",
    "            self.extract_all_stats()\n",
    "        \n",
    "        sizes = self.stats['indel_sizes']\n",
    "        if not sizes:\n",
    "            return pd.DataFrame({'Metric': [], 'Value': []})\n",
    "        \n",
    "        insertions = [s for s in sizes if s > 0]\n",
    "        deletions = [s for s in sizes if s < 0]\n",
    "        \n",
    "        data = {\n",
    "            'Metric': [\n",
    "                'Total INDELs', 'Insertions', 'Deletions',\n",
    "                'Median Size', 'Max Insertion', 'Max Deletion (abs)',\n",
    "                'Mean Insertion Size', 'Mean Deletion Size (abs)'\n",
    "            ],\n",
    "            'Value': [\n",
    "                len(sizes),\n",
    "                len(insertions),\n",
    "                len(deletions),\n",
    "                np.median(np.abs(sizes)) if sizes else 0,\n",
    "                max(insertions) if insertions else 0,\n",
    "                abs(min(deletions)) if deletions else 0,\n",
    "                np.mean(insertions) if insertions else 0,\n",
    "                np.mean(np.abs(deletions)) if deletions else 0\n",
    "            ]\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        df['Value'] = df['Value'].round(2)\n",
    "        return df\n",
    "\n",
    "\n",
    "print('âœ“ VCFBenchmarkStats class defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Extraction\n",
    "\n",
    "Parse VCF filenames and extract statistics from all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_benchmark_filename(vcf_path: Path) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Parse benchmarking VCF filename to extract metadata.\n",
    "    Expected format: Sample_Caller_Model.vcf.gz\n",
    "    \"\"\"\n",
    "    filename = vcf_path.name.replace('.vcf.gz', '').replace('_somatic-only', '')\n",
    "    \n",
    "    # Pattern: Sample_Caller_Model (e.g., H1437_DeepSomatic_HCC1395-model)\n",
    "    pattern = r'^([A-Za-z0-9]+)_([A-Za-z]+)_([A-Za-z0-9-]+)$'\n",
    "    match = re.match(pattern, filename)\n",
    "    \n",
    "    if match:\n",
    "        sample, caller, model = match.groups()\n",
    "        return {\n",
    "            'sample': sample,\n",
    "            'caller': caller,\n",
    "            'model': model,\n",
    "            'filename': vcf_path.name,\n",
    "            'path': vcf_path\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'sample': filename,\n",
    "        'caller': 'Unknown',\n",
    "        'model': 'Unknown',\n",
    "        'filename': vcf_path.name,\n",
    "        'path': vcf_path\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_all_vcf_stats(vcf_paths: List[Path], verbose: bool = True) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Extract statistics from all VCF files.\n",
    "    \n",
    "    Returns:\n",
    "        Dict mapping filename to {'metadata': {...}, 'stats': {...}, 'extractor': VCFBenchmarkStats}\n",
    "    \"\"\"\n",
    "    all_stats = {}\n",
    "    \n",
    "    for i, vcf_path in enumerate(vcf_paths):\n",
    "        if not vcf_path.exists():\n",
    "            if verbose:\n",
    "                print(f'âš ï¸  Skipping missing file: {vcf_path.name}')\n",
    "            continue\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'[{i+1}/{len(vcf_paths)}] Processing {vcf_path.name}...')\n",
    "        \n",
    "        metadata = parse_benchmark_filename(vcf_path)\n",
    "        extractor = VCFBenchmarkStats(vcf_path)\n",
    "        stats = extractor.extract_all_stats()\n",
    "        \n",
    "        all_stats[vcf_path.name] = {\n",
    "            'metadata': metadata,\n",
    "            'stats': stats,\n",
    "            'extractor': extractor\n",
    "        }\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'    â†’ {stats[\"total\"]:,} variants (SNP: {stats[\"snps\"]:,}, INDEL: {stats[\"indels\"]:,})')\n",
    "    \n",
    "    return all_stats\n",
    "\n",
    "\n",
    "# Extract statistics from all VCFs\n",
    "print('Extracting statistics from all VCF files...\\n')\n",
    "all_vcf_stats = extract_all_vcf_stats(valid_vcfs, verbose=True)\n",
    "print(f'\\nâœ“ Processed {len(all_vcf_stats)} VCF files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Whole-Landscape Analysis\n",
    "\n",
    "Cross-sample statistics and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_landscape_summary(all_vcf_stats: Dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create comprehensive summary table with counts and percentages side-by-side.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for filename, data in all_vcf_stats.items():\n",
    "        metadata = data['metadata']\n",
    "        stats = data['stats']\n",
    "        total = stats['total']\n",
    "        \n",
    "        # Calculate PASS count and percentage\n",
    "        pass_count = stats['filters'].get('PASS', 0)\n",
    "        pass_pct = pass_count / total * 100 if total > 0 else 0\n",
    "        \n",
    "        row = {\n",
    "            'Sample': metadata['sample'],\n",
    "            'Model': metadata['model'],\n",
    "            'Total_Variants': total,\n",
    "            'SNP_Count': stats['snps'],\n",
    "            'SNP_Pct': stats['snps'] / total * 100 if total > 0 else 0,\n",
    "            'INDEL_Count': stats['indels'],\n",
    "            'INDEL_Pct': stats['indels'] / total * 100 if total > 0 else 0,\n",
    "            'Other_Count': stats['other'],\n",
    "            'TiTv_Ratio': stats['titv_ratio'],\n",
    "            'PASS_Count': pass_count,\n",
    "            'PASS_Pct': pass_pct,\n",
    "        }\n",
    "        rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Round percentages\n",
    "    pct_cols = [c for c in df.columns if c.endswith('_Pct')]\n",
    "    df[pct_cols] = df[pct_cols].round(2)\n",
    "    df['TiTv_Ratio'] = df['TiTv_Ratio'].round(3)\n",
    "    \n",
    "    # Sort by Sample then Model\n",
    "    df = df.sort_values(['Sample', 'Model']).reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Create landscape summary\n",
    "landscape_df = create_landscape_summary(all_vcf_stats)\n",
    "\n",
    "print('=' * 80)\n",
    "print('LANDSCAPE SUMMARY: All Samples and Models')\n",
    "print('=' * 80)\n",
    "display(landscape_df)\n",
    "\n",
    "# Summary statistics\n",
    "print(f'\\nTotal variants across all files: {landscape_df[\"Total_Variants\"].sum():,}')\n",
    "print(f'Mean Ti/Tv ratio: {landscape_df[\"TiTv_Ratio\"].mean():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Variant Type Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variant type stacked bar chart by sample and model\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=['Variant Counts by Sample', 'Variant Percentages by Sample'],\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "# Prepare data\n",
    "samples = landscape_df['Sample'].unique()\n",
    "models = landscape_df['Model'].unique()\n",
    "\n",
    "colors = {'SNP': '#636EFA', 'INDEL': '#EF553B', 'Other': '#00CC96'}\n",
    "\n",
    "for model in models:\n",
    "    model_data = landscape_df[landscape_df['Model'] == model]\n",
    "    \n",
    "    # Count plot\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            name=f'{model} - SNP',\n",
    "            x=model_data['Sample'],\n",
    "            y=model_data['SNP_Count'],\n",
    "            marker_color=colors['SNP'],\n",
    "            text=model_data['SNP_Count'].apply(lambda x: f'{x:,}'),\n",
    "            textposition='inside',\n",
    "            legendgroup=model,\n",
    "            showlegend=True\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            name=f'{model} - INDEL',\n",
    "            x=model_data['Sample'],\n",
    "            y=model_data['INDEL_Count'],\n",
    "            marker_color=colors['INDEL'],\n",
    "            text=model_data['INDEL_Count'].apply(lambda x: f'{x:,}'),\n",
    "            textposition='inside',\n",
    "            legendgroup=model,\n",
    "            showlegend=True\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Percentage plot\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            name=f'{model} - SNP %',\n",
    "            x=model_data['Sample'],\n",
    "            y=model_data['SNP_Pct'],\n",
    "            marker_color=colors['SNP'],\n",
    "            text=model_data['SNP_Pct'].apply(lambda x: f'{x:.1f}%'),\n",
    "            textposition='inside',\n",
    "            legendgroup=model + '_pct',\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            name=f'{model} - INDEL %',\n",
    "            x=model_data['Sample'],\n",
    "            y=model_data['INDEL_Pct'],\n",
    "            marker_color=colors['INDEL'],\n",
    "            text=model_data['INDEL_Pct'].apply(lambda x: f'{x:.1f}%'),\n",
    "            textposition='inside',\n",
    "            legendgroup=model + '_pct',\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Variant Type Distribution by Sample and Model',\n",
    "    barmode='stack',\n",
    "    height=500,\n",
    "    legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='center', x=0.5)\n",
    ")\n",
    "fig.update_yaxes(title_text='Count', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Percentage (%)', row=1, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Ti/Tv Ratio Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ti/Tv ratio comparison bar chart\n",
    "fig = px.bar(\n",
    "    landscape_df,\n",
    "    x='Sample',\n",
    "    y='TiTv_Ratio',\n",
    "    color='Model',\n",
    "    barmode='group',\n",
    "    title='Ti/Tv Ratio by Sample and Model',\n",
    "    text='TiTv_Ratio',\n",
    "    color_discrete_sequence=px.colors.qualitative.Set2\n",
    ")\n",
    "\n",
    "fig.update_traces(texttemplate='%{text:.2f}', textposition='outside')\n",
    "fig.update_layout(\n",
    "    yaxis_title='Ti/Tv Ratio',\n",
    "    height=450,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "# Add reference lines for expected Ti/Tv ranges\n",
    "fig.add_hline(y=2.0, line_dash='dash', line_color='gray', \n",
    "              annotation_text='Typical WGS (~2.0)', annotation_position='right')\n",
    "fig.add_hline(y=2.8, line_dash='dash', line_color='lightgray',\n",
    "              annotation_text='Typical WES (~2.8)', annotation_position='right')\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Ti/Tv summary statistics\n",
    "print('\\nTi/Tv Ratio Summary:')\n",
    "titv_summary = landscape_df.groupby('Model')['TiTv_Ratio'].agg(['mean', 'std', 'min', 'max']).round(3)\n",
    "display(titv_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Chromosome Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chromosome_matrix(all_vcf_stats: Dict) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Create chromosome distribution matrices (counts and percentages).\n",
    "    Chromosomes are sorted in natural order.\n",
    "    \"\"\"\n",
    "    # Collect all unique chromosomes\n",
    "    all_chroms = set()\n",
    "    for data in all_vcf_stats.values():\n",
    "        all_chroms.update(data['stats']['chromosomes'].keys())\n",
    "    \n",
    "    # Natural sort\n",
    "    sorted_chroms = natural_sort_chromosomes(list(all_chroms))\n",
    "    \n",
    "    # Build matrices\n",
    "    count_data = {}\n",
    "    pct_data = {}\n",
    "    \n",
    "    for filename, data in all_vcf_stats.items():\n",
    "        sample = data['metadata']['sample']\n",
    "        model = data['metadata']['model']\n",
    "        label = f\"{sample}_{model.replace('-model', '')}\"\n",
    "        \n",
    "        total = data['stats']['total']\n",
    "        chrom_counts = data['stats']['chromosomes']\n",
    "        \n",
    "        count_data[label] = {c: chrom_counts.get(c, 0) for c in sorted_chroms}\n",
    "        pct_data[label] = {c: chrom_counts.get(c, 0) / total * 100 if total > 0 else 0 for c in sorted_chroms}\n",
    "    \n",
    "    count_df = pd.DataFrame(count_data).T\n",
    "    count_df = count_df[sorted_chroms]  # Ensure column order\n",
    "    \n",
    "    pct_df = pd.DataFrame(pct_data).T\n",
    "    pct_df = pct_df[sorted_chroms].round(2)\n",
    "    \n",
    "    return count_df, pct_df\n",
    "\n",
    "\n",
    "# Create chromosome matrices\n",
    "chrom_count_df, chrom_pct_df = create_chromosome_matrix(all_vcf_stats)\n",
    "\n",
    "print('Chromosome Distribution (Counts):')\n",
    "display(chrom_count_df.head())\n",
    "\n",
    "print('\\nChromosome Distribution (Percentages):')\n",
    "display(chrom_pct_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chromosome distribution heatmap\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    subplot_titles=['Variant Counts by Chromosome', 'Variant Percentage by Chromosome'],\n",
    "    vertical_spacing=0.15,\n",
    "    row_heights=[0.5, 0.5]\n",
    ")\n",
    "\n",
    "# Count heatmap\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=chrom_count_df.values,\n",
    "        x=chrom_count_df.columns,\n",
    "        y=chrom_count_df.index,\n",
    "        colorscale='Blues',\n",
    "        text=chrom_count_df.values,\n",
    "        texttemplate='%{text:,}',\n",
    "        textfont={'size': 8},\n",
    "        showscale=True,\n",
    "        colorbar=dict(title='Count', len=0.4, y=0.8)\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Percentage heatmap\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=chrom_pct_df.values,\n",
    "        x=chrom_pct_df.columns,\n",
    "        y=chrom_pct_df.index,\n",
    "        colorscale='Greens',\n",
    "        text=chrom_pct_df.values,\n",
    "        texttemplate='%{text:.1f}%',\n",
    "        textfont={'size': 8},\n",
    "        showscale=True,\n",
    "        colorbar=dict(title='%', len=0.4, y=0.25)\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Chromosome Distribution Across Samples (Natural Sort Order)',\n",
    "    height=800,\n",
    "    width=1200\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 FILTER Label Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filter_summary(all_vcf_stats: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Create FILTER label summary across all samples.\"\"\"\n",
    "    # Collect all unique filters\n",
    "    all_filters = set()\n",
    "    for data in all_vcf_stats.values():\n",
    "        all_filters.update(data['stats']['filters'].keys())\n",
    "    \n",
    "    sorted_filters = sorted(all_filters)\n",
    "    \n",
    "    rows = []\n",
    "    for filename, data in all_vcf_stats.items():\n",
    "        metadata = data['metadata']\n",
    "        stats = data['stats']\n",
    "        total = stats['total']\n",
    "        \n",
    "        row = {\n",
    "            'Sample': metadata['sample'],\n",
    "            'Model': metadata['model'],\n",
    "            'Total': total\n",
    "        }\n",
    "        \n",
    "        for filt in sorted_filters:\n",
    "            count = stats['filters'].get(filt, 0)\n",
    "            row[f'{filt}_Count'] = count\n",
    "            row[f'{filt}_Pct'] = count / total * 100 if total > 0 else 0\n",
    "        \n",
    "        rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Round percentages\n",
    "    pct_cols = [c for c in df.columns if c.endswith('_Pct')]\n",
    "    df[pct_cols] = df[pct_cols].round(2)\n",
    "    \n",
    "    return df.sort_values(['Sample', 'Model']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Create FILTER summary\n",
    "filter_df = create_filter_summary(all_vcf_stats)\n",
    "\n",
    "# Display summary\n",
    "print('FILTER Label Summary:')\n",
    "display(filter_df)\n",
    "\n",
    "# Get unique filters for visualization\n",
    "filter_cols = [c.replace('_Count', '') for c in filter_df.columns if c.endswith('_Count')]\n",
    "\n",
    "if len(filter_cols) > 0:\n",
    "    # FILTER distribution grouped bar chart\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    colors = px.colors.qualitative.Set3\n",
    "    \n",
    "    for i, filt in enumerate(filter_cols):\n",
    "        fig.add_trace(go.Bar(\n",
    "            name=filt,\n",
    "            x=[f\"{row['Sample']}_{row['Model'].replace('-model', '')}\" for _, row in filter_df.iterrows()],\n",
    "            y=filter_df[f'{filt}_Count'],\n",
    "            marker_color=colors[i % len(colors)],\n",
    "            text=filter_df[f'{filt}_Pct'].apply(lambda x: f'{x:.1f}%'),\n",
    "            textposition='auto'\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='FILTER Label Distribution (Counts with % Labels)',\n",
    "        xaxis_title='Sample_Model',\n",
    "        yaxis_title='Count',\n",
    "        barmode='group',\n",
    "        height=500,\n",
    "        legend=dict(orientation='h', yanchor='bottom', y=1.02)\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Base Change Spectrum (6-Type Mutation Categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_change_matrix(all_vcf_stats: Dict) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Create base change spectrum matrices (counts and percentages).\"\"\"\n",
    "    categories = ['C>A', 'C>G', 'C>T', 'T>A', 'T>C', 'T>G']\n",
    "    \n",
    "    count_data = {}\n",
    "    pct_data = {}\n",
    "    \n",
    "    for filename, data in all_vcf_stats.items():\n",
    "        sample = data['metadata']['sample']\n",
    "        model = data['metadata']['model']\n",
    "        label = f\"{sample}_{model.replace('-model', '')}\"\n",
    "        \n",
    "        base_changes = data['stats']['base_changes']\n",
    "        snp_total = data['stats']['snps']\n",
    "        \n",
    "        count_data[label] = {c: base_changes.get(c, 0) for c in categories}\n",
    "        pct_data[label] = {c: base_changes.get(c, 0) / snp_total * 100 if snp_total > 0 else 0 for c in categories}\n",
    "    \n",
    "    count_df = pd.DataFrame(count_data).T[categories]\n",
    "    pct_df = pd.DataFrame(pct_data).T[categories].round(2)\n",
    "    \n",
    "    return count_df, pct_df\n",
    "\n",
    "\n",
    "# Create base change matrices\n",
    "bc_count_df, bc_pct_df = create_base_change_matrix(all_vcf_stats)\n",
    "\n",
    "# Display tables with counts and percentages side-by-side\n",
    "print('Base Change Spectrum (Counts):')\n",
    "display(bc_count_df)\n",
    "\n",
    "print('\\nBase Change Spectrum (Percentages of SNPs):')\n",
    "display(bc_pct_df)\n",
    "\n",
    "# Base change spectrum visualization\n",
    "# Standard mutation spectrum colors\n",
    "spectrum_colors = {\n",
    "    'C>A': '#3498db',  # Blue\n",
    "    'C>G': '#000000',  # Black\n",
    "    'C>T': '#e74c3c',  # Red\n",
    "    'T>A': '#95a5a6',  # Gray\n",
    "    'T>C': '#2ecc71',  # Green\n",
    "    'T>G': '#f39c12'   # Orange/Pink\n",
    "}\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=['Base Change Counts', 'Base Change Percentages'],\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "categories = ['C>A', 'C>G', 'C>T', 'T>A', 'T>C', 'T>G']\n",
    "\n",
    "for cat in categories:\n",
    "    # Count bars\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            name=cat,\n",
    "            x=bc_count_df.index,\n",
    "            y=bc_count_df[cat],\n",
    "            marker_color=spectrum_colors[cat],\n",
    "            legendgroup=cat,\n",
    "            showlegend=True\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Percentage bars\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            name=cat,\n",
    "            x=bc_pct_df.index,\n",
    "            y=bc_pct_df[cat],\n",
    "            marker_color=spectrum_colors[cat],\n",
    "            legendgroup=cat,\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title='6-Type Mutation Spectrum Across Samples',\n",
    "    barmode='group',\n",
    "    height=500,\n",
    "    legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='center', x=0.5)\n",
    ")\n",
    "fig.update_xaxes(tickangle=45)\n",
    "fig.update_yaxes(title_text='Count', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Percentage (%)', row=1, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Indel Size Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indel_size_summary(all_vcf_stats: Dict) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Create indel size summary statistics and binned aggregated statistics.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (summary_df, binned_df)\n",
    "    \"\"\"\n",
    "    summary_rows = []\n",
    "    \n",
    "    # Aggregate binned data across all samples\n",
    "    aggregated_insertions = {bin_label: 0 for bin_label in VCFBenchmarkStats.INDEL_SIZE_BINS}\n",
    "    aggregated_deletions = {bin_label: 0 for bin_label in VCFBenchmarkStats.INDEL_SIZE_BINS}\n",
    "    \n",
    "    for filename, data in all_vcf_stats.items():\n",
    "        metadata = data['metadata']\n",
    "        sizes = data['stats']['indel_sizes']\n",
    "        \n",
    "        if not sizes:\n",
    "            continue\n",
    "        \n",
    "        insertions = [s for s in sizes if s > 0]\n",
    "        deletions = [s for s in sizes if s < 0]\n",
    "        \n",
    "        # Get binned stats for this file\n",
    "        extractor = data['extractor']\n",
    "        bin_stats = extractor.indel_stats_by_bin()\n",
    "        \n",
    "        # Aggregate across all samples\n",
    "        for bin_label in VCFBenchmarkStats.INDEL_SIZE_BINS:\n",
    "            aggregated_insertions[bin_label] += bin_stats['insertions'][bin_label]\n",
    "            aggregated_deletions[bin_label] += bin_stats['deletions'][bin_label]\n",
    "        \n",
    "        row = {\n",
    "            'Sample': metadata['sample'],\n",
    "            'Model': metadata['model'],\n",
    "            'Total_INDELs': len(sizes),\n",
    "            'Insertions_Count': len(insertions),\n",
    "            'Insertions_Pct': len(insertions) / len(sizes) * 100 if sizes else 0,\n",
    "            'Deletions_Count': len(deletions),\n",
    "            'Deletions_Pct': len(deletions) / len(sizes) * 100 if sizes else 0,\n",
    "            'Median_Size': np.median(np.abs(sizes)),\n",
    "            'Max_Insertion': max(insertions) if insertions else 0,\n",
    "            'Max_Deletion': abs(min(deletions)) if deletions else 0,\n",
    "            'Mean_Insertion': np.mean(insertions) if insertions else 0,\n",
    "            'Mean_Deletion': np.mean(np.abs(deletions)) if deletions else 0,\n",
    "        }\n",
    "        summary_rows.append(row)\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    \n",
    "    # Round values\n",
    "    for col in ['Insertions_Pct', 'Deletions_Pct', 'Median_Size', 'Mean_Insertion', 'Mean_Deletion']:\n",
    "        if col in summary_df.columns:\n",
    "            summary_df[col] = summary_df[col].round(2)\n",
    "    \n",
    "    summary_df = summary_df.sort_values(['Sample', 'Model']).reset_index(drop=True)\n",
    "    \n",
    "    # Create binned dataframe\n",
    "    total_insertions = sum(aggregated_insertions.values())\n",
    "    total_deletions = sum(aggregated_deletions.values())\n",
    "    \n",
    "    binned_data = []\n",
    "    for bin_label in VCFBenchmarkStats.INDEL_SIZE_BINS:\n",
    "        binned_data.append({\n",
    "            'Size_Bin': bin_label,\n",
    "            'Insertions_Count': aggregated_insertions[bin_label],\n",
    "            'Insertions_Pct': aggregated_insertions[bin_label] / total_insertions * 100 if total_insertions > 0 else 0,\n",
    "            'Deletions_Count': aggregated_deletions[bin_label],\n",
    "            'Deletions_Pct': aggregated_deletions[bin_label] / total_deletions * 100 if total_deletions > 0 else 0\n",
    "        })\n",
    "    \n",
    "    binned_df = pd.DataFrame(binned_data)\n",
    "    binned_df['Insertions_Pct'] = binned_df['Insertions_Pct'].round(2)\n",
    "    binned_df['Deletions_Pct'] = binned_df['Deletions_Pct'].round(2)\n",
    "    \n",
    "    return summary_df, binned_df\n",
    "\n",
    "\n",
    "# Create indel size summary\n",
    "indel_summary_df, indel_binned_df = create_indel_size_summary(all_vcf_stats)\n",
    "\n",
    "print('Indel Size Summary:')\n",
    "display(indel_summary_df)\n",
    "\n",
    "print('\\nIndel Size Distribution by Bins (Aggregated):')\n",
    "display(indel_binned_df)\n",
    "\n",
    "# Indel size distribution: Side-by-side grouped bar chart with counts and percentages\n",
    "fig = go.Figure()\n",
    "\n",
    "bins = indel_binned_df['Size_Bin']\n",
    "insertions_count = indel_binned_df['Insertions_Count']\n",
    "deletions_count = indel_binned_df['Deletions_Count']\n",
    "insertions_pct = indel_binned_df['Insertions_Pct']\n",
    "deletions_pct = indel_binned_df['Deletions_Pct']\n",
    "\n",
    "# Add insertion bars\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Insertions',\n",
    "    x=bins,\n",
    "    y=insertions_count,\n",
    "    marker_color='#00CC96',\n",
    "    text=[f'{count:,}<br>({pct:.1f}%)' for count, pct in zip(insertions_count, insertions_pct)],\n",
    "    textposition='outside',\n",
    "    offsetgroup=0\n",
    "))\n",
    "\n",
    "# Add deletion bars\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Deletions',\n",
    "    x=bins,\n",
    "    y=deletions_count,\n",
    "    marker_color='#EF553B',\n",
    "    text=[f'{count:,}<br>({pct:.1f}%)' for count, pct in zip(deletions_count, deletions_pct)],\n",
    "    textposition='outside',\n",
    "    offsetgroup=1\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Indel Size Distribution by Bins (Aggregated Across All Samples)',\n",
    "    xaxis=dict(title='Size Bin (bp)', tickmode='linear'),\n",
    "    yaxis=dict(title='Count'),\n",
    "    barmode='group',\n",
    "    height=600,\n",
    "    legend=dict(\n",
    "        orientation='h',\n",
    "        yanchor='bottom',\n",
    "        y=1.02,\n",
    "        xanchor='center',\n",
    "        x=0.5\n",
    "    ),\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print('\\nðŸ“Š Indel Statistics Summary:')\n",
    "print(f'Total Insertions: {indel_binned_df[\"Insertions_Count\"].sum():,}')\n",
    "print(f'Total Deletions: {indel_binned_df[\"Deletions_Count\"].sum():,}')\n",
    "print(f'Most common insertion bin: {indel_binned_df.loc[indel_binned_df[\"Insertions_Count\"].idxmax(), \"Size_Bin\"]}')\n",
    "print(f'Most common deletion bin: {indel_binned_df.loc[indel_binned_df[\"Deletions_Count\"].idxmax(), \"Size_Bin\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison: aggregate statistics by model\n",
    "model_comparison = landscape_df.groupby('Model').agg({\n",
    "    'Total_Variants': ['sum', 'mean', 'std'],\n",
    "    'SNP_Count': 'sum',\n",
    "    'SNP_Pct': 'mean',\n",
    "    'INDEL_Count': 'sum',\n",
    "    'INDEL_Pct': 'mean',\n",
    "    'TiTv_Ratio': ['mean', 'std'],\n",
    "    'PASS_Pct': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "model_comparison.columns = ['_'.join(col).strip() for col in model_comparison.columns.values]\n",
    "\n",
    "print('=' * 80)\n",
    "print('MODEL COMPARISON: Aggregate Statistics')\n",
    "print('=' * 80)\n",
    "display(model_comparison)\n",
    "\n",
    "# Model comparison visualization\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=['Total Variants', 'Mean Ti/Tv Ratio', 'Mean SNP Percentage'],\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "models = model_comparison.index.tolist()\n",
    "colors = px.colors.qualitative.Set2[:len(models)]\n",
    "\n",
    "# Total variants\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=models,\n",
    "        y=model_comparison['Total_Variants_sum'],\n",
    "        marker_color=colors,\n",
    "        text=model_comparison['Total_Variants_sum'].apply(lambda x: f'{x:,.0f}'),\n",
    "        textposition='outside'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Mean Ti/Tv\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=models,\n",
    "        y=model_comparison['TiTv_Ratio_mean'],\n",
    "        marker_color=colors,\n",
    "        text=model_comparison['TiTv_Ratio_mean'].apply(lambda x: f'{x:.2f}'),\n",
    "        textposition='outside',\n",
    "        error_y=dict(type='data', array=model_comparison['TiTv_Ratio_std'].values)\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Mean SNP percentage\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=models,\n",
    "        y=model_comparison['SNP_Pct_mean'],\n",
    "        marker_color=colors,\n",
    "        text=model_comparison['SNP_Pct_mean'].apply(lambda x: f'{x:.1f}%'),\n",
    "        textposition='outside'\n",
    "    ),\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Model Comparison Overview',\n",
    "    height=400,\n",
    "    showlegend=False\n",
    ")\n",
    "fig.update_yaxes(title_text='Count', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Ti/Tv Ratio', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Percentage (%)', row=1, col=3)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Per-Sample Analysis\n",
    "\n",
    "Detailed statistics for each individual sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_report(sample_name: str, all_vcf_stats: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create comprehensive report for a single sample across all models.\n",
    "    \"\"\"\n",
    "    # Get all entries for this sample\n",
    "    sample_data = {\n",
    "        fname: data for fname, data in all_vcf_stats.items()\n",
    "        if data['metadata']['sample'] == sample_name\n",
    "    }\n",
    "    \n",
    "    if not sample_data:\n",
    "        return None\n",
    "    \n",
    "    report = {\n",
    "        'sample': sample_name,\n",
    "        'models': {},\n",
    "        'combined': {}\n",
    "    }\n",
    "    \n",
    "    # Per-model statistics\n",
    "    all_chroms = defaultdict(int)\n",
    "    all_base_changes = defaultdict(int)\n",
    "    all_indel_sizes = []\n",
    "    total_snps = 0\n",
    "    total_indels = 0\n",
    "    total_variants = 0\n",
    "    \n",
    "    for fname, data in sample_data.items():\n",
    "        model = data['metadata']['model']\n",
    "        stats = data['stats']\n",
    "        \n",
    "        report['models'][model] = {\n",
    "            'total': stats['total'],\n",
    "            'snps': stats['snps'],\n",
    "            'indels': stats['indels'],\n",
    "            'titv_ratio': stats['titv_ratio'],\n",
    "            'filters': stats['filters'],\n",
    "            'chromosomes': stats['chromosomes'],\n",
    "            'base_changes': stats['base_changes'],\n",
    "            'indel_sizes': stats['indel_sizes']\n",
    "        }\n",
    "        \n",
    "        # Aggregate\n",
    "        total_variants += stats['total']\n",
    "        total_snps += stats['snps']\n",
    "        total_indels += stats['indels']\n",
    "        \n",
    "        for c, count in stats['chromosomes'].items():\n",
    "            all_chroms[c] += count\n",
    "        for bc, count in stats['base_changes'].items():\n",
    "            all_base_changes[bc] += count\n",
    "        all_indel_sizes.extend(stats['indel_sizes'])\n",
    "    \n",
    "    report['combined'] = {\n",
    "        'total_variants': total_variants,\n",
    "        'total_snps': total_snps,\n",
    "        'total_indels': total_indels,\n",
    "        'chromosomes': dict(all_chroms),\n",
    "        'base_changes': dict(all_base_changes),\n",
    "        'indel_sizes': all_indel_sizes\n",
    "    }\n",
    "    \n",
    "    return report\n",
    "\n",
    "\n",
    "def display_sample_analysis(sample_name: str, all_vcf_stats: Dict):\n",
    "    \"\"\"Display comprehensive analysis for a single sample.\"\"\"\n",
    "    report = create_sample_report(sample_name, all_vcf_stats)\n",
    "    \n",
    "    if not report:\n",
    "        print(f'No data found for sample: {sample_name}')\n",
    "        return\n",
    "    \n",
    "    print('=' * 80)\n",
    "    print(f'SAMPLE: {sample_name}')\n",
    "    print('=' * 80)\n",
    "    \n",
    "    # Summary table\n",
    "    summary_rows = []\n",
    "    for model, stats in report['models'].items():\n",
    "        total = stats['total']\n",
    "        summary_rows.append({\n",
    "            'Model': model,\n",
    "            'Total': total,\n",
    "            'SNP_Count': stats['snps'],\n",
    "            'SNP_Pct': f\"{stats['snps']/total*100:.1f}%\" if total > 0 else '0%',\n",
    "            'INDEL_Count': stats['indels'],\n",
    "            'INDEL_Pct': f\"{stats['indels']/total*100:.1f}%\" if total > 0 else '0%',\n",
    "            'Ti/Tv': f\"{stats['titv_ratio']:.3f}\" if stats['titv_ratio'] else 'N/A'\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    print('\\nðŸ“Š Variant Type Summary:')\n",
    "    display(summary_df)\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        specs=[[{'type': 'pie'}, {'type': 'bar'}],\n",
    "               [{'type': 'bar'}, {'type': 'histogram'}]],\n",
    "        subplot_titles=[\n",
    "            'Variant Type Distribution (Combined)',\n",
    "            'Chromosome Distribution (Top 10)',\n",
    "            'Base Change Spectrum',\n",
    "            'Indel Size Distribution'\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    combined = report['combined']\n",
    "    \n",
    "    # 1. Variant type pie chart\n",
    "    fig.add_trace(\n",
    "        go.Pie(\n",
    "            labels=['SNP', 'INDEL'],\n",
    "            values=[combined['total_snps'], combined['total_indels']],\n",
    "            marker_colors=['#636EFA', '#EF553B'],\n",
    "            textinfo='label+percent+value',\n",
    "            hole=0.3\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Chromosome bar chart (top 10, natural sorted)\n",
    "    chroms = natural_sort_chromosomes(list(combined['chromosomes'].keys()))\n",
    "    chrom_counts = [combined['chromosomes'][c] for c in chroms]\n",
    "    total_chrom = sum(chrom_counts)\n",
    "    \n",
    "    # Take top 10 chromosomes by count\n",
    "    chrom_data = sorted(zip(chroms, chrom_counts), key=lambda x: x[1], reverse=True)[:10]\n",
    "    top_chroms, top_counts = zip(*chrom_data) if chrom_data else ([], [])\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=list(top_chroms),\n",
    "            y=list(top_counts),\n",
    "            marker_color='#636EFA',\n",
    "            text=[f'{c:,}<br>({c/total_chrom*100:.1f}%)' for c in top_counts],\n",
    "            textposition='outside'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Base change spectrum\n",
    "    bc_categories = ['C>A', 'C>G', 'C>T', 'T>A', 'T>C', 'T>G']\n",
    "    bc_colors = ['#3498db', '#000000', '#e74c3c', '#95a5a6', '#2ecc71', '#f39c12']\n",
    "    bc_counts = [combined['base_changes'].get(c, 0) for c in bc_categories]\n",
    "    snp_total = combined['total_snps']\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=bc_categories,\n",
    "            y=bc_counts,\n",
    "            marker_color=bc_colors,\n",
    "            text=[f'{c:,}<br>({c/snp_total*100:.1f}%)' if snp_total > 0 else f'{c:,}' for c in bc_counts],\n",
    "            textposition='outside'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Indel size histogram\n",
    "    if combined['indel_sizes']:\n",
    "        fig.add_trace(\n",
    "            go.Histogram(\n",
    "                x=combined['indel_sizes'],\n",
    "                nbinsx=50,\n",
    "                marker_color='#00CC96'\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'Sample Analysis: {sample_name}',\n",
    "        height=800,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return report\n",
    "\n",
    "\n",
    "# Get unique samples\n",
    "unique_samples = list(set(data['metadata']['sample'] for data in all_vcf_stats.values()))\n",
    "print(f'Found {len(unique_samples)} unique samples: {sorted(unique_samples)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate per-sample analysis for each sample\n",
    "sample_reports = {}\n",
    "\n",
    "for sample in sorted(unique_samples):\n",
    "    report = display_sample_analysis(sample, all_vcf_stats)\n",
    "    if report:\n",
    "        sample_reports[sample] = report\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Combined Summary and Export\n",
    "\n",
    "Export all statistics to Excel and CSV formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_all_statistics(\n",
    "    landscape_df: pd.DataFrame,\n",
    "    chrom_count_df: pd.DataFrame,\n",
    "    chrom_pct_df: pd.DataFrame,\n",
    "    bc_count_df: pd.DataFrame,\n",
    "    bc_pct_df: pd.DataFrame,\n",
    "    filter_df: pd.DataFrame,\n",
    "    indel_summary_df: pd.DataFrame,\n",
    "    indel_binned_df: pd.DataFrame,\n",
    "    output_dir: Path\n",
    "):\n",
    "    \"\"\"Export all statistics to multi-sheet Excel file.\"\"\"\n",
    "    \n",
    "    excel_path = output_dir / 'vcf_benchmarking_statistics.xlsx'\n",
    "    \n",
    "    with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "        # Landscape summary\n",
    "        landscape_df.to_excel(writer, sheet_name='Landscape_Summary', index=False)\n",
    "        \n",
    "        # Chromosome distributions\n",
    "        chrom_count_df.to_excel(writer, sheet_name='Chromosome_Counts')\n",
    "        chrom_pct_df.to_excel(writer, sheet_name='Chromosome_Percentages')\n",
    "        \n",
    "        # Base change spectrum\n",
    "        bc_count_df.to_excel(writer, sheet_name='BaseChange_Counts')\n",
    "        bc_pct_df.to_excel(writer, sheet_name='BaseChange_Percentages')\n",
    "        \n",
    "        # FILTER labels\n",
    "        filter_df.to_excel(writer, sheet_name='Filter_Labels', index=False)\n",
    "        \n",
    "        # Indel sizes\n",
    "        indel_summary_df.to_excel(writer, sheet_name='Indel_Summary', index=False)\n",
    "        indel_binned_df.to_excel(writer, sheet_name='Indel_Binned', index=False)\n",
    "    \n",
    "    print(f'âœ“ Excel exported: {excel_path}')\n",
    "    \n",
    "    # Also export individual CSVs\n",
    "    csv_dir = output_dir / 'csv'\n",
    "    csv_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    landscape_df.to_csv(csv_dir / 'landscape_summary.csv', index=False)\n",
    "    chrom_count_df.to_csv(csv_dir / 'chromosome_counts.csv')\n",
    "    chrom_pct_df.to_csv(csv_dir / 'chromosome_percentages.csv')\n",
    "    bc_count_df.to_csv(csv_dir / 'basechange_counts.csv')\n",
    "    bc_pct_df.to_csv(csv_dir / 'basechange_percentages.csv')\n",
    "    filter_df.to_csv(csv_dir / 'filter_labels.csv', index=False)\n",
    "    indel_summary_df.to_csv(csv_dir / 'indel_summary.csv', index=False)\n",
    "    indel_binned_df.to_csv(csv_dir / 'indel_binned.csv', index=False)\n",
    "    \n",
    "    print(f'âœ“ CSV files exported to: {csv_dir}')\n",
    "    \n",
    "    return excel_path\n",
    "\n",
    "\n",
    "# Export all statistics\n",
    "excel_path = export_all_statistics(\n",
    "    landscape_df=landscape_df,\n",
    "    chrom_count_df=chrom_count_df,\n",
    "    chrom_pct_df=chrom_pct_df,\n",
    "    bc_count_df=bc_count_df,\n",
    "    bc_pct_df=bc_pct_df,\n",
    "    filter_df=filter_df,\n",
    "    indel_summary_df=indel_summary_df,\n",
    "    indel_binned_df=indel_binned_df,\n",
    "    output_dir=OUTPUT_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Final Summary Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final summary dashboard\n",
    "print('=' * 80)\n",
    "print('FINAL SUMMARY DASHBOARD')\n",
    "print('=' * 80)\n",
    "\n",
    "# Overall statistics\n",
    "total_variants = landscape_df['Total_Variants'].sum()\n",
    "total_snps = landscape_df['SNP_Count'].sum()\n",
    "total_indels = landscape_df['INDEL_Count'].sum()\n",
    "mean_titv = landscape_df['TiTv_Ratio'].mean()\n",
    "\n",
    "print(f'''\n",
    "ðŸ“Š OVERALL STATISTICS\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "Total VCF Files Analyzed:  {len(all_vcf_stats)}\n",
    "Total Unique Samples:      {len(unique_samples)}\n",
    "Total Models:              {len(landscape_df['Model'].unique())}\n",
    "\n",
    "Total Variants:            {total_variants:,}\n",
    "  â€¢ SNPs:                  {total_snps:,} ({total_snps/total_variants*100:.1f}%)\n",
    "  â€¢ INDELs:                {total_indels:,} ({total_indels/total_variants*100:.1f}%)\n",
    "\n",
    "Mean Ti/Tv Ratio:          {mean_titv:.3f}\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "''')\n",
    "\n",
    "# Cross-tabulation summary\n",
    "print('\\nðŸ“‹ CROSS-TABULATION: Sample Ã— Key Metrics')\n",
    "cross_tab = landscape_df.pivot_table(\n",
    "    index='Sample',\n",
    "    columns='Model',\n",
    "    values=['Total_Variants', 'SNP_Pct', 'TiTv_Ratio'],\n",
    "    aggfunc='first'\n",
    ")\n",
    "display(cross_tab.round(2))\n",
    "\n",
    "# Top chromosomes across all samples\n",
    "print('\\nðŸ§¬ TOP 5 CHROMOSOMES BY VARIANT COUNT (All Samples Combined):')\n",
    "combined_chroms = chrom_count_df.sum().sort_values(ascending=False).head(5)\n",
    "for chrom, count in combined_chroms.items():\n",
    "    pct = count / total_variants * 100\n",
    "    print(f'  {chrom}: {count:,} ({pct:.1f}%)')\n",
    "\n",
    "# Base change spectrum summary\n",
    "print('\\nðŸ”¬ BASE CHANGE SPECTRUM SUMMARY (All Samples Combined):')\n",
    "combined_bc = bc_count_df.sum().sort_values(ascending=False)\n",
    "for bc, count in combined_bc.items():\n",
    "    pct = count / total_snps * 100 if total_snps > 0 else 0\n",
    "    print(f'  {bc}: {count:,} ({pct:.1f}%)')\n",
    "\n",
    "print(f'''\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "âœ“ Analysis complete!\n",
    "âœ“ Results exported to: {OUTPUT_DIR}\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnadnavar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
